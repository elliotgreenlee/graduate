{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os  # filenames\n",
    "from time import time  # timing\n",
    "import numpy as np  # arrays\n",
    "import pandas as pd  # dataframes\n",
    "from tqdm import tqdm  # for loop \"loading bars\"\n",
    "from bs4 import BeautifulSoup  # xml/sgm parsing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # scikit learn tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark imports\n",
    "import findspark\n",
    "findspark.init(\"/Users/elliot/spark\")  # get spark here\n",
    "from pyspark.sql import SparkSession  # session to run spark\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer  # feature extractor\n",
    "from pyspark.ml.classification import NaiveBayes  # classifier\n",
    "from pyspark.sql.functions import udf  # user defined function\n",
    "from pyspark.sql.types import *  # work with various types in the rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/elliot/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/elliot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK imports\n",
    "import nltk  # natural language toolkit\n",
    "from nltk.stem import PorterStemmer  # stem words\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize  # tokenize sentences\n",
    "from nltk.corpus import stopwords  # remove stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the sgm files\n",
    "data_dir = \"data\"\n",
    "data_files = os.listdir(data_dir)\n",
    "sgm_files = [file for file in data_files if file.endswith('.sgm')]\n",
    "sgm_files.sort()  # in place sort for sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe of each non-empty topic and body pair, parsed from the reuters xml/sgm data\n",
    "all_list = []\n",
    "\n",
    "# Deal with each file\n",
    "for sgm_file in sgm_files:\n",
    "    with open(os.path.join(data_dir, sgm_file), 'r') as f:\n",
    "        xml_data = f.read()  \n",
    "        document = BeautifulSoup(xml_data, 'html.parser')\n",
    "        articles = document.find_all('reuters')\n",
    "        \n",
    "        # Deal with each article\n",
    "        for article in articles:\n",
    "            \n",
    "            # Extract the body of the article\n",
    "            body = article.find('body')\n",
    "            \n",
    "            topics = []\n",
    "            \n",
    "            if body is not None:\n",
    "                # Extract the topics from the article\n",
    "                for topic in article.topics.find_all('d'):\n",
    "                    topic = topic.text.split('-')  # Split dashed topics into two separate categories\n",
    "                    \n",
    "                    if topic[0] not in topics:\n",
    "                        topics.append(topic[0])\n",
    "                    if len(topic) == 2:\n",
    "                        if topic[1] not in topics:\n",
    "                            topics.append(topic[1]) \n",
    "                        \n",
    "                for topic in topics:\n",
    "                    all_list.append((topic, body.text))\n",
    "                \n",
    "                                         \n",
    "all_articles = pd.DataFrame.from_records(all_list, columns=['topic', 'body'])\n",
    "\n",
    "# print(all_articles)\n",
    "# print(all_articles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter on the selected topics [10 points]\n",
    "\n",
    "topics = [\"money\", \n",
    "          \"fx\", \n",
    "          \"crude\", \n",
    "          \"grain\", \n",
    "          \"trade\", \n",
    "          \"interest\", \n",
    "          \"wheat\", \n",
    "          \"ship\", \n",
    "          \"corn\", \n",
    "          \"oil\", \n",
    "          \"dlr\", \n",
    "          \"gas\", \n",
    "          \"oilseed\", \n",
    "          \"supply\", \n",
    "          \"sugar\", \n",
    "          \"gnp\", \n",
    "          \"coffee\", \n",
    "          \"veg\", \n",
    "          \"gold\", \n",
    "          \"soybean\", \n",
    "          \"bop\", \n",
    "          \"livestock\", \n",
    "          \"cpi\"]\n",
    "\n",
    "topic_filter = all_articles['topic'].isin(topics)\n",
    "filtered_articles = all_articles[topic_filter]\n",
    "\n",
    "# print(filtered_articles)\n",
    "# print(filtered_articles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6325it [00:00, 11073.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Character removal improvements\n",
    "clean_list = []\n",
    "\n",
    "for index, row in tqdm(filtered_articles.iterrows()):\n",
    "    body = row['body']\n",
    "    body = body.replace('\\n', ' ')\n",
    "    body = body.replace('/', ' ')\n",
    "    body = body.replace('-', ' ')\n",
    "    \n",
    "    clean_list.append((row['topic'], body))\n",
    "    \n",
    "clean_articles = pd.DataFrame.from_records(clean_list, columns=['topic', 'body'])\n",
    "\n",
    "# print(clean_articles)\n",
    "# print(clean_articles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6325it [00:37, 170.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, remove stop words, and stem [10 points]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))  # use nltk english stopwords\n",
    "ps = PorterStemmer()  # use porterstemmer\n",
    "\n",
    "stem_list = []\n",
    "\n",
    "# Iterate over every article iterations\n",
    "for index, row in tqdm(filtered_articles.iterrows()):\n",
    "    \n",
    "    words = word_tokenize(row['body']) # tokenize the article\n",
    "    \n",
    "    # For each word in the article\n",
    "    stemmed = \"\"\n",
    "    for word in words:\n",
    "        if word.lower() not in stop_words:  # remove stop words\n",
    "            stemmed += ps.stem(word.lower()) + \" \"  # stem the word\n",
    "        \n",
    "    stem_list.append((row['topic'], stemmed))\n",
    "    \n",
    "stemmed_articles = pd.DataFrame.from_records(stem_list, columns=['topic', 'body'])\n",
    "\n",
    "# print(stemmed_articles)\n",
    "# print(stemmed_articles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dataframe to a file training_test_data.txt \n",
    "# and print the first ten tuples to another file (example_data.txt) [5 points]\n",
    "\n",
    "stemmed_articles.to_csv(\"training_test_data.txt\", index=False)\n",
    "\n",
    "print_articles = pd.read_csv(\"training_test_data.txt\")\n",
    "print_articles.loc[0:9].to_csv(\"example_data.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The non-spark tfidf took 0.78 seconds\n"
     ]
    }
   ],
   "source": [
    "# Non Spark TF-IDF\n",
    "\n",
    "# read in\n",
    "doc_data = pd.read_csv(\"training_test_data.txt\", dtype={'topic': str, 'body': str})\n",
    "\n",
    "start = time()\n",
    "\n",
    "#tfidf\n",
    "tfidf_vectorizer = TfidfVectorizer()  # min_df = 1, ngram_range=(1, 1)\n",
    "x = tfidf_vectorizer.fit_transform(doc_data['body'])\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(\"The non-spark tfidf took {0:.2f} seconds\".format(end - start))\n",
    "\n",
    "x = x.toarray()\n",
    "\n",
    "# print(x)\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spark tfidf took 1.52 seconds\n"
     ]
    }
   ],
   "source": [
    "# Spark TF-IDF\n",
    "\n",
    "# start spark session\n",
    "spark = SparkSession.builder.appName(\"NewsClassification\").getOrCreate()\n",
    "\n",
    "# read in\n",
    "df = spark.read.csv(\"training_test_data.txt\", header=True) \n",
    "\n",
    "start = time()\n",
    "\n",
    "# re-tokenize\n",
    "tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"tokens\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "# tf\n",
    "hashingTF = HashingTF(inputCol='tokens', outputCol='term_frequency')\n",
    "df = hashingTF.transform(df)\n",
    "\n",
    "# idf\n",
    "idf = IDF(inputCol='term_frequency', outputCol='tfidf', minDocFreq=1)\n",
    "idfModel = idf.fit(df)\n",
    "df = idfModel.transform(df)\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(\"The spark tfidf took {0:.2f} seconds\".format(end - start))\n",
    "\n",
    "# df.show()\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long do the non-spark and spark versions take to create the tfidf representation? [20 points]\n",
    "\n",
    "How do the two representations compare? [5 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the labels to be integers\n",
    "topics = [\"money\", \n",
    "          \"fx\", \n",
    "          \"crude\", \n",
    "          \"grain\", \n",
    "          \"trade\", \n",
    "          \"interest\", \n",
    "          \"wheat\", \n",
    "          \"ship\", \n",
    "          \"corn\", \n",
    "          \"oil\", \n",
    "          \"dlr\", \n",
    "          \"gas\", \n",
    "          \"oilseed\", \n",
    "          \"supply\", \n",
    "          \"sugar\", \n",
    "          \"gnp\", \n",
    "          \"coffee\", \n",
    "          \"veg\", \n",
    "          \"gold\", \n",
    "          \"soybean\", \n",
    "          \"bop\", \n",
    "          \"livestock\", \n",
    "          \"cpi\"]\n",
    "\n",
    "# Encode the labels\n",
    "def label_encoder(topic):\n",
    "    if topic in topics:\n",
    "        return topics.index(topic)\n",
    "    else:\n",
    "        return -1\n",
    "        \n",
    "label_encoder_udf = udf(label_encoder, IntegerType())\n",
    "df = df.withColumn('topic_int', label_encoder_udf('topic'))\n",
    "\n",
    "# df.show()\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into Training-Validation, and Testing Data\n",
    "df = df.select('tfidf', 'topic_int')\n",
    "\n",
    "training_validating, testing = df.randomSplit([0.9, 0.1], seed=1937)\n",
    "\n",
    "# training_validating.show()\n",
    "# testing.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% data testing accuracy: 0.328\n",
      "50% data testing accuracy: 0.3392\n",
      "50% data testing accuracy: 0.3568\n",
      "50% data testing accuracy: 0.3376\n",
      "50% data testing accuracy: 0.344\n",
      "50% data testing accuracy: 0.3296\n",
      "50% data testing accuracy: 0.3184\n",
      "50% data testing accuracy: 0.312\n",
      "50% data testing accuracy: 0.3312\n",
      "50% data testing accuracy: 0.3232\n",
      "50% data average validation accuracy: 0.3386784109632125\n",
      "60% training data accuracy: 0.3104\n",
      "60% training data accuracy: 0.32\n",
      "60% training data accuracy: 0.3184\n",
      "60% training data accuracy: 0.3088\n",
      "60% training data accuracy: 0.304\n",
      "60% training data accuracy: 0.3184\n",
      "60% training data accuracy: 0.312\n",
      "60% training data accuracy: 0.3152\n",
      "60% training data accuracy: 0.312\n",
      "60% training data accuracy: 0.3248\n",
      "60% data average validation accuracy: 0.32674124798596893\n",
      "70% training data accuracy: 0.3088\n",
      "70% training data accuracy: 0.3008\n",
      "70% training data accuracy: 0.3216\n",
      "70% training data accuracy: 0.3104\n",
      "70% training data accuracy: 0.312\n",
      "70% training data accuracy: 0.3136\n",
      "70% training data accuracy: 0.304\n",
      "70% training data accuracy: 0.2992\n",
      "70% training data accuracy: 0.2896\n",
      "70% training data accuracy: 0.3216\n",
      "70% data average validation accuracy: 0.3212850208375943\n"
     ]
    }
   ],
   "source": [
    "# Split the rest of the data 3 different ways (56/44, 67/33, and 78/22)\n",
    "# Train each split 10 times using NaiveBayes.train(), and save the 30 models. \n",
    "# Report the average accuracy per split [20 points]\n",
    "# Report the model with the best accuracy of the 30 on the testing dataset, and what that accuracy is [15 points]\n",
    "\n",
    "avg = 0\n",
    "models_50 = []\n",
    "for i in range(0, 10):\n",
    "    \n",
    "    # Split the data into training and validating Data\n",
    "    training, validating = training_validating.randomSplit([0.56, 0.44])\n",
    "\n",
    "    # Apply naive bayes\n",
    "    nb = NaiveBayes(featuresCol='tfidf', labelCol='topic_int', predictionCol=\"NB_pred\", probabilityCol=\"NB_prob\", rawPredictionCol=\"NB_rawPred\")\n",
    "    nbModel = nb.fit(training)\n",
    "    models_50.append(nbModel)\n",
    "\n",
    "    # Get validation accuracy\n",
    "    predicting = nbModel.transform(validating)\n",
    "    total = predicting.count()\n",
    "    correct = predicting.where(predicting['topic_int'] == predicting['NB_pred']).count()\n",
    "    avg += correct/total\n",
    "    \n",
    "    # Get testing accuracy\n",
    "    predicting = nbModel.transform(testing)\n",
    "    total = predicting.count()\n",
    "    correct = predicting.where(predicting['topic_int'] == predicting['NB_pred']).count()\n",
    "    print(\"50% data testing accuracy:\", correct/total)\n",
    "    \n",
    "print(\"50% data average validation accuracy: {}\".format(avg / 10.0))\n",
    "\n",
    "avg = 0\n",
    "models_60 = []\n",
    "for i in range(0, 10):\n",
    "    \n",
    "    # Split the data into training and validating Data\n",
    "    training, validating = training_validating.randomSplit([0.67, 0.33])\n",
    "    \n",
    "    # Apply naive bayes\n",
    "    nb = NaiveBayes(featuresCol='tfidf', labelCol='topic_int', predictionCol=\"NB_pred\", probabilityCol=\"NB_prob\", rawPredictionCol=\"NB_rawPred\")\n",
    "    nbModel = nb.fit(training)\n",
    "    models_60.append(nbModel)\n",
    "\n",
    "    # Get validation accuracy\n",
    "    predicting = nbModel.transform(validating)\n",
    "    total = predicting.count()\n",
    "    correct = predicting.where(predicting['topic_int'] == predicting['NB_pred']).count()\n",
    "    avg += correct/total\n",
    "    \n",
    "    # Get testing accuracy\n",
    "    predicting = nbModel.transform(testing)\n",
    "    total = predicting.count()\n",
    "    correct = predicting.where(predicting['topic_int'] == predicting['NB_pred']).count()\n",
    "    print(\"60% training data accuracy:\", correct/total)\n",
    "\n",
    "print(\"60% data average validation accuracy: {}\".format(avg / 10.0))\n",
    "    \n",
    "avg = 0\n",
    "models_70 = []\n",
    "for i in range(0, 10):\n",
    "    \n",
    "    # Split the data into training and validating\n",
    "    training, validating = training_validating.randomSplit([0.78, 0.22])\n",
    "\n",
    "    # Apply naive bayes\n",
    "    nb = NaiveBayes(featuresCol='tfidf', labelCol='topic_int', predictionCol=\"NB_pred\", probabilityCol=\"NB_prob\", rawPredictionCol=\"NB_rawPred\")\n",
    "    nbModel = nb.fit(training)\n",
    "    models_70.append(nbModel)\n",
    "\n",
    "    # Get validation accuracy\n",
    "    predicting = nbModel.transform(validating)\n",
    "    total = predicting.count()\n",
    "    correct = predicting.where(predicting['topic_int'] == predicting['NB_pred']).count()\n",
    "    avg += correct/total\n",
    "    \n",
    "    # Get testing accuracy\n",
    "    predicting = nbModel.transform(testing)\n",
    "    total = predicting.count()\n",
    "    correct = predicting.where(predicting['topic_int'] == predicting['NB_pred']).count()\n",
    "    print(\"70% training data accuracy:\", correct/total)\n",
    "\n",
    "print(\"70% data average validation accuracy: {}\".format(avg / 10.0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What trends occur when the amount of training data increases? [15 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
